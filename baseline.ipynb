{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497819, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer \n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from parapply import parapply\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv(sys.path[0]+'/train.csv',sep=',',dtype=object)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data,n_jobs = 1):\n",
    "\n",
    "    for character in string.punctuation:\n",
    "        data['name_1'] = data['name_1'].apply(lambda x: str(x).replace(character, ''))\n",
    "        data['name_2'] = data['name_2'].apply(lambda x: str(x).replace(character, ''))\n",
    "\n",
    "    data['lang_1'] = parapply(data['name_1'],lambda x: detect(x),n_jobs=n_jobs)\n",
    "    data['lang_2'] = parapply(data['name_2'],lambda x: detect(x),n_jobs=n_jobs)\n",
    "\n",
    "    lang_ru_indexes =data[data['lang_1']=='ru'].index\n",
    "    print(lang_ru_indexes)\n",
    "    if len(list(lang_ru_indexes))==0:\n",
    "        lang_other_indexes = data.index\n",
    "    else:\n",
    "        lang_other_indexes =data.index.drop(lang_ru_indexes)\n",
    "    stemmer_ru = SnowballStemmer(\"russian\")\n",
    "    stemmer_other = SnowballStemmer(\"english\")\n",
    "\n",
    "    if len(list(lang_ru_indexes))!=0:\n",
    "        data.loc[lang_ru_indexes,'name_1_stemmed'] = parapply(data.loc[lang_ru_indexes,'name_1'],lambda x: stemmer_ru.stem(x),n_jobs=4)\n",
    "        data.loc[lang_other_indexes,'name_1_stemmed'] = parapply(data.loc[lang_other_indexes,'name_1'],lambda x: stemmer_other.stem(x),n_jobs=4)\n",
    "    else:\n",
    "        data.loc[lang_other_indexes,'name_1_stemmed'] = parapply(data.loc[lang_other_indexes,'name_1'],lambda x: stemmer_other.stem(x),n_jobs=4)\n",
    "\n",
    "    lang_ru_indexes =data[data['lang_2']=='ru'].index\n",
    "    print(lang_ru_indexes)\n",
    "    if len(list(lang_ru_indexes))==0:\n",
    "        lang_other_indexes = data.index\n",
    "    else:\n",
    "        lang_other_indexes =data.index.drop(lang_ru_indexes)\n",
    "    stemmer_ru = SnowballStemmer(\"russian\")\n",
    "    stemmer_other = SnowballStemmer(\"english\")\n",
    "\n",
    "    if len(list(lang_ru_indexes))!=0:\n",
    "        data.loc[lang_ru_indexes,'name_2_stemmed'] = parapply(data.loc[lang_ru_indexes,'name_2'],lambda x: stemmer_ru.stem(x),n_jobs=n_jobs)\n",
    "        data.loc[lang_other_indexes,'name_2_stemmed'] = parapply(data.loc[lang_other_indexes,'name_2'],lambda x: stemmer_other.stem(x),n_jobs=n_jobs)\n",
    "    else:\n",
    "        data.loc[lang_other_indexes,'name_2_stemmed'] = parapply(data.loc[lang_other_indexes,'name_2'],lambda x: stemmer_other.stem(x),n_jobs=n_jobs)\n",
    "   \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "\tdot_product = np.dot(a, b)\n",
    "\tnorm_a = np.linalg.norm(a)\n",
    "\tnorm_b = np.linalg.norm(b)\n",
    "\treturn dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77256/528708173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_77256/3993243617.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(data, n_jobs)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name_2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang_1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang_2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/parapply/__init__.py\u001b[0m in \u001b[0;36mparapply\u001b[0;34m(obj, fun, axis, n_jobs, n_chunks, backend, verbose)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0msplit_obj_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_srs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loky'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_obj_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = preprocessing(data,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(sys.path[0]+'/CompanyNamesSimularity_dataset.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(sys.path[0]+'/CompanyNamesSimularity_dataset.csv',sep=';',dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 4M words\n",
      "Number of words:  16376\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   90969 lr:  0.000000 avg.loss:  0.554888 ETA:   0h 0m 0s  7.5% words/sec/thread:   92970 lr:  0.092471 avg.loss:  1.638084 ETA:   0h 0m23s 17.5% words/sec/thread:   91275 lr:  0.082497 avg.loss:  1.121155 ETA:   0h 0m21s 19.5% words/sec/thread:   91576 lr:  0.080489 avg.loss:  1.068077 ETA:   0h 0m20s 29.0% words/sec/thread:   92016 lr:  0.070988 avg.loss:  0.908398 ETA:   0h 0m18s 40.6% words/sec/thread:   91545 lr:  0.059436 avg.loss:  0.800276 ETA:   0h 0m15s  0h 0m11s\n",
      "Read 4M words\n",
      "Number of words:  16376\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  153554 lr:  0.000000 avg.loss:  0.553253 ETA:   0h 0m 0s 53.8% words/sec/thread:  159130 lr:  0.023119 avg.loss:  0.715448 ETA:   0h 0m 9s 69.5% words/sec/thread:  156309 lr:  0.015272 avg.loss:  0.634221 ETA:   0h 0m 6s 79.9% words/sec/thread:  155292 lr:  0.010065 avg.loss:  0.599934 ETA:   0h 0m 4s 95.3% words/sec/thread:  154244 lr:  0.002357 avg.loss:  0.563265 ETA:   0h 0m 1s 97.5% words/sec/thread:  154112 lr:  0.001225 avg.loss:  0.558371 ETA:   0h 0m 0s 99.8% words/sec/thread:  153996 lr:  0.000091 avg.loss:  0.553651 ETA:   0h 0m 0s\n",
      "Read 4M words\n",
      "Number of words:  16376\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  198113 lr: -0.000000 avg.loss:  0.919945 ETA:   0h 0m 0s  1.4% words/sec/thread:  215272 lr:  0.009862 avg.loss:  3.733787 ETA:   0h 0m21s  3.7% words/sec/thread:  215489 lr:  0.009633 avg.loss:  2.951290 ETA:   0h 0m21s  5.9% words/sec/thread:  214845 lr:  0.009405 avg.loss:  2.703772 ETA:   0h 0m20s  8.2% words/sec/thread:  214434 lr:  0.009178 avg.loss:  2.541424 ETA:   0h 0m20s 10.3% words/sec/thread:  210720 lr:  0.008968 avg.loss:  2.414254 ETA:   0h 0m20s 16.6% words/sec/thread:  205184 lr:  0.008340 avg.loss:  2.083429 ETA:   0h 0m19s 21.2% words/sec/thread:  203022 lr:  0.007881 avg.loss:  1.895227 ETA:   0h 0m18s 23.3% words/sec/thread:  202524 lr:  0.007671 avg.loss:  1.822399 ETA:   0h 0m17s 25.4% words/sec/thread:  202092 lr:  0.007460 avg.loss:  1.756574 ETA:   0h 0m17s 35.5% words/sec/thread:  200693 lr:  0.006452 avg.loss:  1.508746 ETA:   0h 0m15s 37.6% words/sec/thread:  200461 lr:  0.006243 avg.loss:  1.467824 ETA:   0h 0m14s 44.3% words/sec/thread:  199817 lr:  0.005574 avg.loss:  1.357495 ETA:   0h 0m13s 49.7% words/sec/thread:  199485 lr:  0.005029 avg.loss:  1.282445 ETA:   0h 0m11s 63.5% words/sec/thread:  198880 lr:  0.003646 avg.loss:  1.137918 ETA:   0h 0m 8s 65.6% words/sec/thread:  198833 lr:  0.003436 avg.loss:  1.120262 ETA:   0h 0m 8s 72.3% words/sec/thread:  198627 lr:  0.002766 avg.loss:  1.069325 ETA:   0h 0m 6s 76.5% words/sec/thread:  198556 lr:  0.002346 avg.loss:  1.041223 ETA:   0h 0m 5s 78.6% words/sec/thread:  198498 lr:  0.002137 avg.loss:  1.027763 ETA:   0h 0m 5s 91.6% words/sec/thread:  198271 lr:  0.000837 avg.loss:  0.956717 ETA:   0h 0m 1s 93.7% words/sec/thread:  198261 lr:  0.000626 avg.loss:  0.946885 ETA:   0h 0m 1s100.0% words/sec/thread:  198112 lr:  0.000000 avg.loss:  0.919945 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_1 = fasttext.train_unsupervised(input=sys.path[0]+'/train_strings.csv',model=\"skipgram\", minn=2, maxn=5, dim=300,epoch=5, lr=0.1,thread=10)\n",
    "model_2 = fasttext.train_unsupervised(input=sys.path[0]+'/train_strings.csv',model=\"skipgram\", minn=2, maxn=5, dim=150,epoch=7, lr=0.05,thread=10)\n",
    "model_3 = fasttext.train_unsupervised(input=sys.path[0]+'/train_strings.csv',model=\"skipgram\", minn=2, maxn=5, dim=100,epoch=10, lr=0.01,thread=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save_model(sys.path[0]+'/model_1.bin')\n",
    "model_2.save_model(sys.path[0]+'/model_2.bin')\n",
    "model_3.save_model(sys.path[0]+'/model_3.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_1 = fasttext.load_model(sys.path[0]+'/model_1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['is_duplicate'] = dataset['is_duplicate'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 8)\n"
     ]
    }
   ],
   "source": [
    "dataset_0 = dataset[dataset['is_duplicate']==0].sample(n=3600)\n",
    "dataset_1 = dataset[dataset['is_duplicate']==1].sample(n=3600)\n",
    "\n",
    "dataset_train_test = dataset_0.append(dataset_1).reset_index(drop=True)\n",
    "\n",
    "print(dataset_train_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name_1 = np.zeros((dataset_train_test.shape[0],300),dtype='float32')\n",
    "embeddings_name_2 = np.zeros((dataset_train_test.shape[0],300),dtype='float32')\n",
    "\n",
    "for i,sentence in enumerate(dataset_train_test['name_1_stemmed']):\n",
    "    embeddings_name_1[i,:] = model_1.get_sentence_vector(sentence)\n",
    "for i,sentence in enumerate(dataset_train_test['name_2_stemmed']):\n",
    "    embeddings_name_2[i,:] = model_1.get_sentence_vector(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 600)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_1 = np.hstack((embeddings_name_1,embeddings_name_2))\n",
    "embeddings_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>lang_1</th>\n",
       "      <th>lang_2</th>\n",
       "      <th>name_1_stemmed</th>\n",
       "      <th>name_2_stemmed</th>\n",
       "      <th>cos_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469360</td>\n",
       "      <td>I B International</td>\n",
       "      <td>Guangzhou City Banger Fu Shoes Co Ltd</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>i b intern</td>\n",
       "      <td>guangzhou city banger fu shoes co ltd</td>\n",
       "      <td>0.445330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462512</td>\n",
       "      <td>Anchor Logistics</td>\n",
       "      <td>Eastway Logistics Usa Llc</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>anchor logist</td>\n",
       "      <td>eastway logistics usa llc</td>\n",
       "      <td>0.501545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>412189</td>\n",
       "      <td>Fast Rite International Inc</td>\n",
       "      <td>Faj International</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>fast rite international inc</td>\n",
       "      <td>faj intern</td>\n",
       "      <td>0.534506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66926</td>\n",
       "      <td>Textiles Omnes SA</td>\n",
       "      <td>Shohagpur Textile Mills Ltd</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>en</td>\n",
       "      <td>textiles omnes sa</td>\n",
       "      <td>shohagpur textile mills ltd</td>\n",
       "      <td>0.511297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219893</td>\n",
       "      <td>Russell Corp Aus PL</td>\n",
       "      <td>Acs International Sa De Cv</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>russell corp aus pl</td>\n",
       "      <td>acs international sa de cv</td>\n",
       "      <td>0.432505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>422155</td>\n",
       "      <td>Sika S A</td>\n",
       "      <td>Ultra Plastik</td>\n",
       "      <td>0</td>\n",
       "      <td>tl</td>\n",
       "      <td>et</td>\n",
       "      <td>sika s a</td>\n",
       "      <td>ultra plastik</td>\n",
       "      <td>0.465515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>438135</td>\n",
       "      <td>Samsung India Electronics Pvt Ltd</td>\n",
       "      <td>AG India Retail Pvt Ltd</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>id</td>\n",
       "      <td>samsung india electronics pvt ltd</td>\n",
       "      <td>ag india retail pvt ltd</td>\n",
       "      <td>0.843018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>349072</td>\n",
       "      <td>Репсол</td>\n",
       "      <td>TERMOPRENE POLYMERS</td>\n",
       "      <td>0</td>\n",
       "      <td>mk</td>\n",
       "      <td>vi</td>\n",
       "      <td>репсол</td>\n",
       "      <td>termoprene polym</td>\n",
       "      <td>0.683315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>172252</td>\n",
       "      <td>Guangzhou Jian Shun Metal Products Co Ltd</td>\n",
       "      <td>Texmate Products Co Ltd</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>guangzhou jian shun metal products co ltd</td>\n",
       "      <td>texmate products co ltd</td>\n",
       "      <td>0.839532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>91962</td>\n",
       "      <td>PROBIGALP</td>\n",
       "      <td>Total Petrochemicals France SA</td>\n",
       "      <td>0</td>\n",
       "      <td>tl</td>\n",
       "      <td>en</td>\n",
       "      <td>probigalp</td>\n",
       "      <td>total petrochemicals france sa</td>\n",
       "      <td>0.547178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pair_id                                     name_1  \\\n",
       "0  469360                          I B International   \n",
       "1  462512                           Anchor Logistics   \n",
       "2  412189                Fast Rite International Inc   \n",
       "3   66926                          Textiles Omnes SA   \n",
       "4  219893                        Russell Corp Aus PL   \n",
       "5  422155                                   Sika S A   \n",
       "6  438135          Samsung India Electronics Pvt Ltd   \n",
       "7  349072                                     Репсол   \n",
       "8  172252  Guangzhou Jian Shun Metal Products Co Ltd   \n",
       "9   91962                                  PROBIGALP   \n",
       "\n",
       "                                  name_2  is_duplicate lang_1 lang_2  \\\n",
       "0  Guangzhou City Banger Fu Shoes Co Ltd             0     en     en   \n",
       "1              Eastway Logistics Usa Llc             0     en     en   \n",
       "2                      Faj International             0     en     en   \n",
       "3            Shohagpur Textile Mills Ltd             0     fr     en   \n",
       "4             Acs International Sa De Cv             0     en     en   \n",
       "5                          Ultra Plastik             0     tl     et   \n",
       "6                AG India Retail Pvt Ltd             0     en     id   \n",
       "7                    TERMOPRENE POLYMERS             0     mk     vi   \n",
       "8                Texmate Products Co Ltd             0     en     en   \n",
       "9         Total Petrochemicals France SA             0     tl     en   \n",
       "\n",
       "                              name_1_stemmed  \\\n",
       "0                                 i b intern   \n",
       "1                              anchor logist   \n",
       "2                fast rite international inc   \n",
       "3                          textiles omnes sa   \n",
       "4                        russell corp aus pl   \n",
       "5                                   sika s a   \n",
       "6          samsung india electronics pvt ltd   \n",
       "7                                     репсол   \n",
       "8  guangzhou jian shun metal products co ltd   \n",
       "9                                  probigalp   \n",
       "\n",
       "                          name_2_stemmed  cos_distance  \n",
       "0  guangzhou city banger fu shoes co ltd      0.445330  \n",
       "1              eastway logistics usa llc      0.501545  \n",
       "2                             faj intern      0.534506  \n",
       "3            shohagpur textile mills ltd      0.511297  \n",
       "4             acs international sa de cv      0.432505  \n",
       "5                          ultra plastik      0.465515  \n",
       "6                ag india retail pvt ltd      0.843018  \n",
       "7                       termoprene polym      0.683315  \n",
       "8                texmate products co ltd      0.839532  \n",
       "9         total petrochemicals france sa      0.547178  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simularity(embeddings):\n",
    "    distances = np.zeros(embeddings.shape[0])\n",
    "\n",
    "    for i, sentence in enumerate(embeddings):\n",
    "        vector_i = sentence[0:300]\n",
    "        vector_j = sentence[300:]\n",
    "        distances[i] = cos_sim(vector_i,vector_j)\n",
    "\n",
    "    return distances\n",
    "\n",
    "cos_distance = simularity(embeddings_1)\n",
    "\n",
    "dataset_train_test['cos_distance'] = cos_distance\n",
    "\n",
    "dataset_train_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_590</th>\n",
       "      <th>emb_591</th>\n",
       "      <th>emb_592</th>\n",
       "      <th>emb_593</th>\n",
       "      <th>emb_594</th>\n",
       "      <th>emb_595</th>\n",
       "      <th>emb_596</th>\n",
       "      <th>emb_597</th>\n",
       "      <th>emb_598</th>\n",
       "      <th>emb_599</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001832</td>\n",
       "      <td>-0.005357</td>\n",
       "      <td>-0.018089</td>\n",
       "      <td>-0.037893</td>\n",
       "      <td>0.061595</td>\n",
       "      <td>0.016472</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>-0.078075</td>\n",
       "      <td>0.054644</td>\n",
       "      <td>0.051844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035557</td>\n",
       "      <td>-0.040607</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>-0.033571</td>\n",
       "      <td>-0.054104</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>0.032292</td>\n",
       "      <td>-0.034382</td>\n",
       "      <td>-0.031298</td>\n",
       "      <td>-0.072496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007306</td>\n",
       "      <td>-0.010997</td>\n",
       "      <td>-0.048597</td>\n",
       "      <td>-0.007986</td>\n",
       "      <td>0.030631</td>\n",
       "      <td>-0.004835</td>\n",
       "      <td>0.032116</td>\n",
       "      <td>-0.047467</td>\n",
       "      <td>0.043619</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009196</td>\n",
       "      <td>-0.048925</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>-0.068814</td>\n",
       "      <td>-0.049209</td>\n",
       "      <td>-0.038378</td>\n",
       "      <td>0.017818</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>-0.010693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039517</td>\n",
       "      <td>-0.015658</td>\n",
       "      <td>-0.010704</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>-0.033919</td>\n",
       "      <td>0.074195</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>-0.072259</td>\n",
       "      <td>0.079542</td>\n",
       "      <td>0.031767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007273</td>\n",
       "      <td>-0.084578</td>\n",
       "      <td>0.029172</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>-0.063051</td>\n",
       "      <td>-0.060403</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>-0.074181</td>\n",
       "      <td>0.049591</td>\n",
       "      <td>-0.045154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.033255</td>\n",
       "      <td>-0.054866</td>\n",
       "      <td>-0.024194</td>\n",
       "      <td>0.010445</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>0.084975</td>\n",
       "      <td>0.066347</td>\n",
       "      <td>-0.105140</td>\n",
       "      <td>0.080603</td>\n",
       "      <td>0.057089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023507</td>\n",
       "      <td>-0.048227</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>-0.018319</td>\n",
       "      <td>-0.019748</td>\n",
       "      <td>-0.022851</td>\n",
       "      <td>-0.011163</td>\n",
       "      <td>-0.029804</td>\n",
       "      <td>-0.046149</td>\n",
       "      <td>-0.029158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.009719</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>-0.069145</td>\n",
       "      <td>-0.004493</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>0.036689</td>\n",
       "      <td>-0.038998</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>0.080446</td>\n",
       "      <td>0.027402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.012361</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>-0.020170</td>\n",
       "      <td>-0.036760</td>\n",
       "      <td>0.015192</td>\n",
       "      <td>-0.011975</td>\n",
       "      <td>0.018698</td>\n",
       "      <td>-0.005453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.012938</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>-0.044147</td>\n",
       "      <td>-0.020030</td>\n",
       "      <td>-0.022612</td>\n",
       "      <td>0.051612</td>\n",
       "      <td>0.056126</td>\n",
       "      <td>-0.087974</td>\n",
       "      <td>0.078466</td>\n",
       "      <td>-0.047529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013546</td>\n",
       "      <td>-0.084729</td>\n",
       "      <td>-0.034007</td>\n",
       "      <td>-0.056249</td>\n",
       "      <td>-0.074073</td>\n",
       "      <td>-0.040985</td>\n",
       "      <td>0.019617</td>\n",
       "      <td>-0.022484</td>\n",
       "      <td>0.040190</td>\n",
       "      <td>-0.038749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.051184</td>\n",
       "      <td>-0.081450</td>\n",
       "      <td>0.035143</td>\n",
       "      <td>0.014622</td>\n",
       "      <td>0.076482</td>\n",
       "      <td>-0.001165</td>\n",
       "      <td>-0.093878</td>\n",
       "      <td>0.069112</td>\n",
       "      <td>0.078263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100199</td>\n",
       "      <td>-0.047080</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>-0.018604</td>\n",
       "      <td>-0.043540</td>\n",
       "      <td>-0.031729</td>\n",
       "      <td>-0.003587</td>\n",
       "      <td>0.010222</td>\n",
       "      <td>0.021703</td>\n",
       "      <td>0.008686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.016338</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>-0.089981</td>\n",
       "      <td>-0.012576</td>\n",
       "      <td>0.056740</td>\n",
       "      <td>0.077999</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>-0.074421</td>\n",
       "      <td>0.057563</td>\n",
       "      <td>-0.008481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>-0.080450</td>\n",
       "      <td>-0.031769</td>\n",
       "      <td>-0.019681</td>\n",
       "      <td>-0.045459</td>\n",
       "      <td>0.024883</td>\n",
       "      <td>-0.036445</td>\n",
       "      <td>-0.048044</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>0.025275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.013925</td>\n",
       "      <td>-0.032157</td>\n",
       "      <td>-0.049879</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0.016370</td>\n",
       "      <td>0.076424</td>\n",
       "      <td>-0.011697</td>\n",
       "      <td>-0.060263</td>\n",
       "      <td>0.052594</td>\n",
       "      <td>0.014901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062435</td>\n",
       "      <td>-0.038717</td>\n",
       "      <td>-0.047688</td>\n",
       "      <td>-0.032733</td>\n",
       "      <td>-0.020969</td>\n",
       "      <td>-0.008708</td>\n",
       "      <td>0.015790</td>\n",
       "      <td>-0.011955</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>-0.020582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.002780</td>\n",
       "      <td>0.019226</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.004623</td>\n",
       "      <td>0.020593</td>\n",
       "      <td>0.048343</td>\n",
       "      <td>0.071213</td>\n",
       "      <td>-0.091670</td>\n",
       "      <td>-0.023990</td>\n",
       "      <td>-0.008614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013756</td>\n",
       "      <td>-0.031798</td>\n",
       "      <td>-0.003775</td>\n",
       "      <td>-0.016691</td>\n",
       "      <td>-0.059477</td>\n",
       "      <td>-0.046844</td>\n",
       "      <td>-0.006594</td>\n",
       "      <td>-0.057879</td>\n",
       "      <td>0.028604</td>\n",
       "      <td>0.046187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
       "0 -0.001832 -0.005357 -0.018089 -0.037893  0.061595  0.016472  0.015916   \n",
       "1  0.007306 -0.010997 -0.048597 -0.007986  0.030631 -0.004835  0.032116   \n",
       "2 -0.039517 -0.015658 -0.010704  0.003947 -0.033919  0.074195  0.016669   \n",
       "3 -0.033255 -0.054866 -0.024194  0.010445  0.007902  0.084975  0.066347   \n",
       "4 -0.009719  0.035695 -0.069145 -0.004493  0.010645  0.036689 -0.038998   \n",
       "5 -0.012938  0.000636 -0.044147 -0.020030 -0.022612  0.051612  0.056126   \n",
       "6 -0.061584 -0.051184 -0.081450  0.035143  0.014622  0.076482 -0.001165   \n",
       "7 -0.016338  0.027901 -0.089981 -0.012576  0.056740  0.077999  0.052903   \n",
       "8  0.013925 -0.032157 -0.049879  0.012025  0.016370  0.076424 -0.011697   \n",
       "9 -0.002780  0.019226 -0.093982 -0.004623  0.020593  0.048343  0.071213   \n",
       "\n",
       "      emb_7     emb_8     emb_9  ...   emb_590   emb_591   emb_592   emb_593  \\\n",
       "0 -0.078075  0.054644  0.051844  ... -0.035557 -0.040607  0.007681 -0.033571   \n",
       "1 -0.047467  0.043619 -0.044867  ... -0.009196 -0.048925  0.014472 -0.068814   \n",
       "2 -0.072259  0.079542  0.031767  ... -0.007273 -0.084578  0.029172  0.029100   \n",
       "3 -0.105140  0.080603  0.057089  ... -0.023507 -0.048227  0.010708 -0.018319   \n",
       "4 -0.001504  0.080446  0.027402  ...  0.013631  0.012361  0.017501  0.041185   \n",
       "5 -0.087974  0.078466 -0.047529  ...  0.013546 -0.084729 -0.034007 -0.056249   \n",
       "6 -0.093878  0.069112  0.078263  ... -0.100199 -0.047080  0.016594 -0.018604   \n",
       "7 -0.074421  0.057563 -0.008481  ...  0.020491 -0.080450 -0.031769 -0.019681   \n",
       "8 -0.060263  0.052594  0.014901  ... -0.062435 -0.038717 -0.047688 -0.032733   \n",
       "9 -0.091670 -0.023990 -0.008614  ... -0.013756 -0.031798 -0.003775 -0.016691   \n",
       "\n",
       "    emb_594   emb_595   emb_596   emb_597   emb_598   emb_599  \n",
       "0 -0.054104 -0.006219  0.032292 -0.034382 -0.031298 -0.072496  \n",
       "1 -0.049209 -0.038378  0.017818  0.000500 -0.002472 -0.010693  \n",
       "2 -0.063051 -0.060403  0.026766 -0.074181  0.049591 -0.045154  \n",
       "3 -0.019748 -0.022851 -0.011163 -0.029804 -0.046149 -0.029158  \n",
       "4 -0.020170 -0.036760  0.015192 -0.011975  0.018698 -0.005453  \n",
       "5 -0.074073 -0.040985  0.019617 -0.022484  0.040190 -0.038749  \n",
       "6 -0.043540 -0.031729 -0.003587  0.010222  0.021703  0.008686  \n",
       "7 -0.045459  0.024883 -0.036445 -0.048044  0.007728  0.025275  \n",
       "8 -0.020969 -0.008708  0.015790 -0.011955 -0.006279 -0.020582  \n",
       "9 -0.059477 -0.046844 -0.006594 -0.057879  0.028604  0.046187  \n",
       "\n",
       "[10 rows x 600 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['emb_'+str(i) for i in range(embeddings_1.shape[1])]\n",
    "embeddings_1_pd = pd.DataFrame(data=embeddings_1,columns=cols)\n",
    "embeddings_1_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pair_id', 'name_1', 'name_2', 'is_duplicate', 'lang_1', 'lang_2',\n",
       "       'name_1_stemmed', 'name_2_stemmed', 'cos_distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 605)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test = dataset_train_test[['pair_id', 'name_1', 'name_2','cos_distance','is_duplicate']]\n",
    "dataset_train_test = pd.concat((dataset_train_test,embeddings_1_pd),axis=1)\n",
    "\n",
    "dataset_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_distance</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_590</th>\n",
       "      <th>emb_591</th>\n",
       "      <th>emb_592</th>\n",
       "      <th>emb_593</th>\n",
       "      <th>emb_594</th>\n",
       "      <th>emb_595</th>\n",
       "      <th>emb_596</th>\n",
       "      <th>emb_597</th>\n",
       "      <th>emb_598</th>\n",
       "      <th>emb_599</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.445330</td>\n",
       "      <td>-0.001832</td>\n",
       "      <td>-0.005357</td>\n",
       "      <td>-0.018089</td>\n",
       "      <td>-0.037893</td>\n",
       "      <td>0.061595</td>\n",
       "      <td>0.016472</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>-0.078075</td>\n",
       "      <td>0.054644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035557</td>\n",
       "      <td>-0.040607</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>-0.033571</td>\n",
       "      <td>-0.054104</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>0.032292</td>\n",
       "      <td>-0.034382</td>\n",
       "      <td>-0.031298</td>\n",
       "      <td>-0.072496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.501545</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>-0.010997</td>\n",
       "      <td>-0.048597</td>\n",
       "      <td>-0.007986</td>\n",
       "      <td>0.030631</td>\n",
       "      <td>-0.004835</td>\n",
       "      <td>0.032116</td>\n",
       "      <td>-0.047467</td>\n",
       "      <td>0.043619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009196</td>\n",
       "      <td>-0.048925</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>-0.068814</td>\n",
       "      <td>-0.049209</td>\n",
       "      <td>-0.038378</td>\n",
       "      <td>0.017818</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>-0.010693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.534506</td>\n",
       "      <td>-0.039517</td>\n",
       "      <td>-0.015658</td>\n",
       "      <td>-0.010704</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>-0.033919</td>\n",
       "      <td>0.074195</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>-0.072259</td>\n",
       "      <td>0.079542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007273</td>\n",
       "      <td>-0.084578</td>\n",
       "      <td>0.029172</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>-0.063051</td>\n",
       "      <td>-0.060403</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>-0.074181</td>\n",
       "      <td>0.049591</td>\n",
       "      <td>-0.045154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511297</td>\n",
       "      <td>-0.033255</td>\n",
       "      <td>-0.054866</td>\n",
       "      <td>-0.024194</td>\n",
       "      <td>0.010445</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>0.084975</td>\n",
       "      <td>0.066347</td>\n",
       "      <td>-0.105140</td>\n",
       "      <td>0.080603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023507</td>\n",
       "      <td>-0.048227</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>-0.018319</td>\n",
       "      <td>-0.019748</td>\n",
       "      <td>-0.022851</td>\n",
       "      <td>-0.011163</td>\n",
       "      <td>-0.029804</td>\n",
       "      <td>-0.046149</td>\n",
       "      <td>-0.029158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.432505</td>\n",
       "      <td>-0.009719</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>-0.069145</td>\n",
       "      <td>-0.004493</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>0.036689</td>\n",
       "      <td>-0.038998</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>0.080446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.012361</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>-0.020170</td>\n",
       "      <td>-0.036760</td>\n",
       "      <td>0.015192</td>\n",
       "      <td>-0.011975</td>\n",
       "      <td>0.018698</td>\n",
       "      <td>-0.005453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>0.430169</td>\n",
       "      <td>-0.073264</td>\n",
       "      <td>-0.015716</td>\n",
       "      <td>-0.099982</td>\n",
       "      <td>0.019117</td>\n",
       "      <td>0.058611</td>\n",
       "      <td>0.150183</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>-0.121214</td>\n",
       "      <td>0.082927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061353</td>\n",
       "      <td>-0.035592</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>-0.050721</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>-0.012953</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>-0.022435</td>\n",
       "      <td>0.036606</td>\n",
       "      <td>0.001171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>0.777375</td>\n",
       "      <td>0.037517</td>\n",
       "      <td>0.019593</td>\n",
       "      <td>-0.045194</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>0.084254</td>\n",
       "      <td>0.039886</td>\n",
       "      <td>-0.004177</td>\n",
       "      <td>-0.036135</td>\n",
       "      <td>0.024022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013701</td>\n",
       "      <td>-0.084223</td>\n",
       "      <td>-0.012969</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>-0.058086</td>\n",
       "      <td>-0.045581</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>-0.094162</td>\n",
       "      <td>0.019973</td>\n",
       "      <td>0.024804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7197</th>\n",
       "      <td>0.943115</td>\n",
       "      <td>-0.044412</td>\n",
       "      <td>0.006240</td>\n",
       "      <td>-0.050735</td>\n",
       "      <td>0.031415</td>\n",
       "      <td>0.009828</td>\n",
       "      <td>0.059827</td>\n",
       "      <td>-0.009289</td>\n",
       "      <td>-0.008394</td>\n",
       "      <td>0.022051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044809</td>\n",
       "      <td>-0.074404</td>\n",
       "      <td>-0.062049</td>\n",
       "      <td>-0.010760</td>\n",
       "      <td>-0.064863</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.006718</td>\n",
       "      <td>-0.007186</td>\n",
       "      <td>0.044937</td>\n",
       "      <td>0.036848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>0.816286</td>\n",
       "      <td>-0.070342</td>\n",
       "      <td>-0.029253</td>\n",
       "      <td>-0.086381</td>\n",
       "      <td>0.021393</td>\n",
       "      <td>0.041321</td>\n",
       "      <td>0.105658</td>\n",
       "      <td>0.022926</td>\n",
       "      <td>-0.053025</td>\n",
       "      <td>0.057728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029877</td>\n",
       "      <td>-0.067691</td>\n",
       "      <td>-0.004638</td>\n",
       "      <td>-0.027852</td>\n",
       "      <td>-0.058365</td>\n",
       "      <td>-0.011397</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>-0.023680</td>\n",
       "      <td>0.073649</td>\n",
       "      <td>-0.024641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7199</th>\n",
       "      <td>0.573496</td>\n",
       "      <td>-0.009605</td>\n",
       "      <td>-0.024988</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>-0.005028</td>\n",
       "      <td>0.012748</td>\n",
       "      <td>0.071528</td>\n",
       "      <td>0.043441</td>\n",
       "      <td>-0.011311</td>\n",
       "      <td>0.028905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052377</td>\n",
       "      <td>-0.078158</td>\n",
       "      <td>0.039176</td>\n",
       "      <td>-0.023409</td>\n",
       "      <td>-0.080394</td>\n",
       "      <td>-0.025364</td>\n",
       "      <td>0.070647</td>\n",
       "      <td>-0.016102</td>\n",
       "      <td>0.060614</td>\n",
       "      <td>-0.061601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7200 rows × 601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cos_distance     emb_0     emb_1     emb_2     emb_3     emb_4  \\\n",
       "0         0.445330 -0.001832 -0.005357 -0.018089 -0.037893  0.061595   \n",
       "1         0.501545  0.007306 -0.010997 -0.048597 -0.007986  0.030631   \n",
       "2         0.534506 -0.039517 -0.015658 -0.010704  0.003947 -0.033919   \n",
       "3         0.511297 -0.033255 -0.054866 -0.024194  0.010445  0.007902   \n",
       "4         0.432505 -0.009719  0.035695 -0.069145 -0.004493  0.010645   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "7195      0.430169 -0.073264 -0.015716 -0.099982  0.019117  0.058611   \n",
       "7196      0.777375  0.037517  0.019593 -0.045194  0.013140  0.084254   \n",
       "7197      0.943115 -0.044412  0.006240 -0.050735  0.031415  0.009828   \n",
       "7198      0.816286 -0.070342 -0.029253 -0.086381  0.021393  0.041321   \n",
       "7199      0.573496 -0.009605 -0.024988 -0.010753 -0.005028  0.012748   \n",
       "\n",
       "         emb_5     emb_6     emb_7     emb_8  ...   emb_590   emb_591  \\\n",
       "0     0.016472  0.015916 -0.078075  0.054644  ... -0.035557 -0.040607   \n",
       "1    -0.004835  0.032116 -0.047467  0.043619  ... -0.009196 -0.048925   \n",
       "2     0.074195  0.016669 -0.072259  0.079542  ... -0.007273 -0.084578   \n",
       "3     0.084975  0.066347 -0.105140  0.080603  ... -0.023507 -0.048227   \n",
       "4     0.036689 -0.038998 -0.001504  0.080446  ...  0.013631  0.012361   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "7195  0.150183  0.004204 -0.121214  0.082927  ... -0.061353 -0.035592   \n",
       "7196  0.039886 -0.004177 -0.036135  0.024022  ... -0.013701 -0.084223   \n",
       "7197  0.059827 -0.009289 -0.008394  0.022051  ... -0.044809 -0.074404   \n",
       "7198  0.105658  0.022926 -0.053025  0.057728  ... -0.029877 -0.067691   \n",
       "7199  0.071528  0.043441 -0.011311  0.028905  ...  0.052377 -0.078158   \n",
       "\n",
       "       emb_592   emb_593   emb_594   emb_595   emb_596   emb_597   emb_598  \\\n",
       "0     0.007681 -0.033571 -0.054104 -0.006219  0.032292 -0.034382 -0.031298   \n",
       "1     0.014472 -0.068814 -0.049209 -0.038378  0.017818  0.000500 -0.002472   \n",
       "2     0.029172  0.029100 -0.063051 -0.060403  0.026766 -0.074181  0.049591   \n",
       "3     0.010708 -0.018319 -0.019748 -0.022851 -0.011163 -0.029804 -0.046149   \n",
       "4     0.017501  0.041185 -0.020170 -0.036760  0.015192 -0.011975  0.018698   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7195  0.003717 -0.050721  0.000943 -0.012953  0.006160 -0.022435  0.036606   \n",
       "7196 -0.012969  0.009103 -0.058086 -0.045581  0.013086 -0.094162  0.019973   \n",
       "7197 -0.062049 -0.010760 -0.064863  0.010117  0.006718 -0.007186  0.044937   \n",
       "7198 -0.004638 -0.027852 -0.058365 -0.011397  0.005090 -0.023680  0.073649   \n",
       "7199  0.039176 -0.023409 -0.080394 -0.025364  0.070647 -0.016102  0.060614   \n",
       "\n",
       "       emb_599  \n",
       "0    -0.072496  \n",
       "1    -0.010693  \n",
       "2    -0.045154  \n",
       "3    -0.029158  \n",
       "4    -0.005453  \n",
       "...        ...  \n",
       "7195  0.001171  \n",
       "7196  0.024804  \n",
       "7197  0.036848  \n",
       "7198 -0.024641  \n",
       "7199 -0.061601  \n",
       "\n",
       "[7200 rows x 601 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test.drop(['pair_id', 'name_1', 'name_2','is_duplicate'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "7195    1\n",
       "7196    1\n",
       "7197    1\n",
       "7198    1\n",
       "7199    1\n",
       "Name: is_duplicate, Length: 7200, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_test['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_test.to_csv('dataset_train_test.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, y_train, y_test = train_test_split(dataset_train_test.drop(['pair_id', 'name_1', 'name_2','is_duplicate'],axis=1),dataset_train_test['is_duplicate'],\n",
    "test_size=0.3,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      2513\n",
      "           1       0.91      0.92      0.92      2527\n",
      "\n",
      "    accuracy                           0.92      5040\n",
      "   macro avg       0.92      0.92      0.92      5040\n",
      "weighted avg       0.92      0.92      0.92      5040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train,y_train)\n",
    "y_train_pred = logreg.predict(train)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1087\n",
      "           1       0.91      0.93      0.92      1073\n",
      "\n",
      "    accuracy                           0.92      2160\n",
      "   macro avg       0.92      0.92      0.92      2160\n",
      "weighted avg       0.92      0.92      0.92      2160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = logreg.predict(test)\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      2513\n",
      "           1       0.91      0.92      0.92      2527\n",
      "\n",
      "    accuracy                           0.92      5040\n",
      "   macro avg       0.92      0.92      0.92      5040\n",
      "weighted avg       0.92      0.92      0.92      5040\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1087\n",
      "           1       0.91      0.93      0.92      1073\n",
      "\n",
      "    accuracy                           0.92      2160\n",
      "   macro avg       0.92      0.92      0.92      2160\n",
      "weighted avg       0.92      0.92      0.92      2160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2 = fasttext.load_model(sys.path[0]+'/model_2.bin')\n",
    "\n",
    "dataset_train_test = dataset_0.append(dataset_1).reset_index(drop=True)\n",
    "\n",
    "embeddings_name_1 = np.zeros((dataset_train_test.shape[0],300),dtype='float32')\n",
    "embeddings_name_2 = np.zeros((dataset_train_test.shape[0],300),dtype='float32')\n",
    "\n",
    "for i,sentence in enumerate(dataset_train_test['name_1_stemmed']):\n",
    "    embeddings_name_1[i,:] = model_1.get_sentence_vector(sentence)\n",
    "for i,sentence in enumerate(dataset_train_test['name_2_stemmed']):\n",
    "    embeddings_name_2[i,:] = model_1.get_sentence_vector(sentence)\n",
    "\n",
    "embeddings_2 = np.hstack((embeddings_name_1,embeddings_name_2))\n",
    "\n",
    "\n",
    "\n",
    "cos_distance = simularity(embeddings_2)\n",
    "\n",
    "dataset_train_test['cos_distance'] = cos_distance\n",
    "\n",
    "cols = ['emb_'+str(i) for i in range(embeddings_2.shape[1])]\n",
    "embeddings_2_pd = pd.DataFrame(data=embeddings_2,columns=cols)\n",
    "\n",
    "dataset_train_test = dataset_train_test[['pair_id', 'name_1', 'name_2','cos_distance','is_duplicate']]\n",
    "dataset_train_test = pd.concat((dataset_train_test,embeddings_2_pd),axis=1)\n",
    "\n",
    "train, test, y_train, y_test = train_test_split(dataset_train_test.drop(['pair_id', 'name_1', 'name_2','is_duplicate'],axis=1),dataset_train_test['is_duplicate'],\n",
    "test_size=0.3,shuffle=True,random_state=42)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train,y_train)\n",
    "\n",
    "y_train_pred = logreg.predict(train)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = logreg.predict(test)\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      2513\n",
      "           1       0.91      0.92      0.92      2527\n",
      "\n",
      "    accuracy                           0.92      5040\n",
      "   macro avg       0.92      0.92      0.92      5040\n",
      "weighted avg       0.92      0.92      0.92      5040\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1087\n",
      "           1       0.91      0.93      0.92      1073\n",
      "\n",
      "    accuracy                           0.92      2160\n",
      "   macro avg       0.92      0.92      0.92      2160\n",
      "weighted avg       0.92      0.92      0.92      2160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3 = fasttext.load_model(sys.path[0]+'/model_3.bin')\n",
    "\n",
    "dataset_train_test = dataset_0.append(dataset_1).reset_index(drop=True)\n",
    "\n",
    "embeddings_name_1 = np.zeros((dataset_train_test.shape[0],300),dtype='float32')\n",
    "embeddings_name_3 = np.zeros((dataset_train_test.shape[0],300),dtype='float32')\n",
    "\n",
    "for i,sentence in enumerate(dataset_train_test['name_1_stemmed']):\n",
    "    embeddings_name_1[i,:] = model_1.get_sentence_vector(sentence)\n",
    "for i,sentence in enumerate(dataset_train_test['name_2_stemmed']):\n",
    "    embeddings_name_3[i,:] = model_1.get_sentence_vector(sentence)\n",
    "\n",
    "embeddings_3 = np.hstack((embeddings_name_1,embeddings_name_3))\n",
    "\n",
    "\n",
    "\n",
    "cos_distance = simularity(embeddings_3)\n",
    "\n",
    "dataset_train_test['cos_distance'] = cos_distance\n",
    "\n",
    "cols = ['emb_'+str(i) for i in range(embeddings_3.shape[1])]\n",
    "embeddings_3_pd = pd.DataFrame(data=embeddings_3,columns=cols)\n",
    "\n",
    "dataset_train_test = dataset_train_test[['pair_id', 'name_1', 'name_2','cos_distance','is_duplicate']]\n",
    "dataset_train_test = pd.concat((dataset_train_test,embeddings_3_pd),axis=1)\n",
    "\n",
    "train, test, y_train, y_test = train_test_split(dataset_train_test.drop(['pair_id', 'name_1', 'name_2','is_duplicate'],axis=1),dataset_train_test['is_duplicate'],\n",
    "test_size=0.3,shuffle=True,random_state=42)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train,y_train)\n",
    "\n",
    "y_train_pred = logreg.predict(train)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = logreg.predict(test)\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
